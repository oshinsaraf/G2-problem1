{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --uprgrade selenium"
      ],
      "metadata": {
        "id": "n6F4iC2HY_6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to extract description from the startup link\n",
        "def extract_description(startup_link):\n",
        "    # Send a GET request to the startup link\n",
        "    response = requests.get(startup_link)\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the div containing the description\n",
        "    description_div = soup.find('div', class_='startup__description')\n",
        "\n",
        "    # Extract the description text\n",
        "    description = description_div.text.strip() if description_div else ''\n",
        "\n",
        "    return description\n",
        "\n",
        "# Function to scrape startup cards from a given page URL\n",
        "def scrape_startup_cards(url, csv_writer):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the startup cards\n",
        "    startup_cards = soup.find_all('div', class_='startupCard')\n",
        "\n",
        "    # Check if there are no startup cards on this page\n",
        "    if not startup_cards:\n",
        "        return False\n",
        "\n",
        "    # Loop through each startup card and extract information\n",
        "    for card in startup_cards:\n",
        "        # Extract the name of the startup\n",
        "        startup_name_element = card.find('a', class_='block whitespace-nowrap text-ellipsis overflow-hidden font-medium')\n",
        "        startup_name = startup_name_element.text\n",
        "\n",
        "        # Generate the startup link\n",
        "        startup_link = f\"https://betalist.com{startup_name_element['href']}\"\n",
        "\n",
        "        # Extract the description\n",
        "        description = extract_description(startup_link)\n",
        "\n",
        "        # Write the data to the CSV file\n",
        "        csv_writer.writerow({'Name': startup_name, 'Description': description})\n",
        "\n",
        "    return True\n",
        "\n",
        "# Base URL of the webpage to scrape\n",
        "base_url = 'https://betalist.com/topics/startups'\n",
        "page_number = 1\n",
        "\n",
        "# Create a CSV file to save the data\n",
        "csv_file_path = 'startup_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['Name', 'Description']\n",
        "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    csv_writer.writeheader()\n",
        "\n",
        "    # Scrape startup cards from all pages\n",
        "    while True:\n",
        "        page_url = f\"{base_url}?page={page_number}\"\n",
        "        print(f\"Scraping page {page_number}...\")\n",
        "        if not scrape_startup_cards(page_url, csv_writer):\n",
        "            print(\"No more startup cards found. Stopping the scraping process.\")\n",
        "            break\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "print(f\"Data has been successfully saved to '{csv_file_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNjZEum8YpmA",
        "outputId": "4f1a1a04-676a-4183-dd0c-5610d4970374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the main dataset CSV file\n",
        "main_dataset_path = \"/content/g2_finaldata.csv\"  # Update with the path to your main dataset CSV file\n",
        "main_df = pd.read_csv(main_dataset_path)\n",
        "\n",
        "# Load the dataset with fields \"name\" and \"description\"\n",
        "provided_dataset_path = \"/content/startup_data.csv\"  # Update with the path to your provided dataset CSV file\n",
        "provided_df = pd.read_csv(provided_dataset_path)\n",
        "\n",
        "# Merge the two datasets based on name and description\n",
        "merged_df = pd.merge(main_df, provided_df, on=[\"name\", \"description\"], how=\"left\", indicator=True)\n",
        "\n",
        "# Filter out the products that are only in the main dataset\n",
        "missing_products = merged_df.loc[merged_df[\"_merge\"] == \"left_only\", main_df.columns]\n",
        "\n",
        "# Display the list of missing products\n",
        "print(\"Missing Products:\")\n",
        "print(missing_products)\n"
      ],
      "metadata": {
        "id": "VhorUbttfvmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513b64e7-6a7c-4620-c642-cacfd58a00cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Products:\n",
            "      product_type                            name  \\\n",
            "0         Software  Simple Custom Reports for Jira   \n",
            "1         Software  Ampliz Healthcare Intelligence   \n",
            "2         Provider                      MAKRAM SRL   \n",
            "3         Software                          Isogen   \n",
            "4         Software                     GetJobsDone   \n",
            "...            ...                             ...   \n",
            "43335     Software                      Klient PSA   \n",
            "43336     Software                          Midesk   \n",
            "43337     Software                          WRKTOP   \n",
            "43338     Software                           Later   \n",
            "43339     Software                   Apideck Unify   \n",
            "\n",
            "                                             description  \n",
            "0      A powerful tool that allows you to create cust...  \n",
            "1      Ampliz, the leading healthcare data provider, ...  \n",
            "2                                                    NaN  \n",
            "3      Isogen is the world's leading solution for the...  \n",
            "4      GetJobsDone is a platform with more than 10,00...  \n",
            "...                                                  ...  \n",
            "43335  Klient helps companies transform service deliv...  \n",
            "43336  Midesk is an innovative market intelligence so...  \n",
            "43337  The DESKLESS WORKFORCE on the field can COLLAB...  \n",
            "43338  Visually curate, plan, and manage your social ...  \n",
            "43339                                                NaN  \n",
            "\n",
            "[43336 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the main dataset CSV file\n",
        "main_dataset_path = \"/content/g2_finaldata.csv\"  # Update with the path to your main dataset CSV file\n",
        "main_df = pd.read_csv(main_dataset_path)\n",
        "\n",
        "# Load the dataset with fields \"name\" and \"description\"\n",
        "provided_dataset_path = \"/content/startup_data.csv\"  # Update with the path to your provided dataset CSV file\n",
        "provided_df = pd.read_csv(provided_dataset_path)\n",
        "\n",
        "# Extract the unique products from both datasets\n",
        "main_products = set(zip(main_df[\"name\"], main_df[\"description\"]))\n",
        "provided_products = set(zip(provided_df[\"name\"], provided_df[\"description\"]))\n",
        "\n",
        "# Find the products in the provided dataset that are not in the main dataset\n",
        "missing_products = provided_products - main_products\n",
        "\n",
        "# Convert the missing products set to a DataFrame\n",
        "missing_products_df = pd.DataFrame(list(missing_products), columns=[\"name\", \"description\"])\n",
        "\n",
        "# Save the list of missing products to a CSV file\n",
        "missing_products_csv_path = \"missing_products.csv\"\n",
        "missing_products_df.to_csv(missing_products_csv_path, index=False)\n",
        "\n",
        "# Display the list of missing products\n",
        "print(\"Missing Products:\")\n",
        "print(missing_products_df)\n",
        "\n",
        "print(f\"The list of missing products has been saved to '{missing_products_csv_path}'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Mpkt13PxvN5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87e449c-2683-44db-e73c-56f2f9592612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Products:\n",
            "                         name  \\\n",
            "0     Ok2 Smart Shortener URL   \n",
            "1             Marketing Stack   \n",
            "2                   SESMetric   \n",
            "3                    Inoffice   \n",
            "4              Get Tech Press   \n",
            "...                       ...   \n",
            "3628        Quaderno Checkout   \n",
            "3629                   Opesta   \n",
            "3630                     Qoop   \n",
            "3631                 ROCKY.AI   \n",
            "3632                   Modify   \n",
            "\n",
            "                                            description  \n",
            "0     Ok2 Smart Shortener URL is Build, Customize & ...  \n",
            "1     Marketing Stack is the easiest way to filter t...  \n",
            "2     SES Metricg was started with main mission to p...  \n",
            "3     Inoffice is a collaboration tool for remote de...  \n",
            "4     Get Tech Press solves the pain of getting pres...  \n",
            "...                                                 ...  \n",
            "3628  Quaderno Checkout is a simple widget to provid...  \n",
            "3629  Opesta allows users to create detailed marketi...  \n",
            "3630  Qoop is a gamified social Q&A where users earn...  \n",
            "3631  ROCKY.AI is your simple path to high-performin...  \n",
            "3632  Modify helps software teams create, build and ...  \n",
            "\n",
            "[3633 rows x 2 columns]\n",
            "The list of missing products has been saved to 'missing_products.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the main dataset CSV file\n",
        "main_dataset_path = \"/content/g2_finaldata.csv\"  # Update with the path to your main dataset CSV file\n",
        "main_df = pd.read_csv(main_dataset_path)\n",
        "\n",
        "# Load the dataset with fields \"name\" and \"description\"\n",
        "provided_dataset_path = \"/content/startup_data.csv\"  # Update with the path to your provided dataset CSV file\n",
        "provided_df = pd.read_csv(provided_dataset_path)\n",
        "\n",
        "# Extract the unique products from both datasets\n",
        "main_products = set(zip(main_df[\"name\"], main_df[\"description\"]))\n",
        "provided_products = set(zip(provided_df[\"name\"], provided_df[\"description\"]))\n",
        "\n",
        "# Find the products in the provided dataset that are not in the main dataset\n",
        "missing_products = provided_products - main_products\n",
        "\n",
        "# Find the common products between both datasets\n",
        "common_products = provided_products.intersection(main_products)\n",
        "\n",
        "# Convert the missing products set to a DataFrame\n",
        "missing_products_df = pd.DataFrame(list(missing_products), columns=[\"name\", \"description\"])\n",
        "\n",
        "# Convert the common products set to a DataFrame\n",
        "common_products_df = pd.DataFrame(list(common_products), columns=[\"name\", \"description\"])\n",
        "\n",
        "# Save the list of missing products to a CSV file\n",
        "missing_products_csv_path = \"missing_productstry.csv\"\n",
        "missing_products_df.to_csv(missing_products_csv_path, index=False)\n",
        "\n",
        "# Save the list of common products to a CSV file\n",
        "common_products_csv_path = \"common_productstry.csv\"\n",
        "common_products_df.to_csv(common_products_csv_path, index=False)\n",
        "\n",
        "# Display the list of missing products\n",
        "print(\"Missing Products:\")\n",
        "print(missing_products_df)\n",
        "\n",
        "print(f\"The list of missing products has been saved to '{missing_products_csv_path}'.\")\n",
        "\n",
        "# Display the list of common products\n",
        "print(\"\\nCommon Products:\")\n",
        "print(common_products_df)\n",
        "\n",
        "print(f\"The list of common products has been saved to '{common_products_csv_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJondCanyOQd",
        "outputId": "157ff145-fe2e-47c1-c559-06d17d8a3a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Products:\n",
            "                         name  \\\n",
            "0     Ok2 Smart Shortener URL   \n",
            "1             Marketing Stack   \n",
            "2                   SESMetric   \n",
            "3                    Inoffice   \n",
            "4              Get Tech Press   \n",
            "...                       ...   \n",
            "3628        Quaderno Checkout   \n",
            "3629                   Opesta   \n",
            "3630                     Qoop   \n",
            "3631                 ROCKY.AI   \n",
            "3632                   Modify   \n",
            "\n",
            "                                            description  \n",
            "0     Ok2 Smart Shortener URL is Build, Customize & ...  \n",
            "1     Marketing Stack is the easiest way to filter t...  \n",
            "2     SES Metricg was started with main mission to p...  \n",
            "3     Inoffice is a collaboration tool for remote de...  \n",
            "4     Get Tech Press solves the pain of getting pres...  \n",
            "...                                                 ...  \n",
            "3628  Quaderno Checkout is a simple widget to provid...  \n",
            "3629  Opesta allows users to create detailed marketi...  \n",
            "3630  Qoop is a gamified social Q&A where users earn...  \n",
            "3631  ROCKY.AI is your simple path to high-performin...  \n",
            "3632  Modify helps software teams create, build and ...  \n",
            "\n",
            "[3633 rows x 2 columns]\n",
            "The list of missing products has been saved to 'missing_productstry.csv'.\n",
            "\n",
            "Common Products:\n",
            "        name                                        description\n",
            "0    Inodash  Inodash helps you easily generate new business...\n",
            "1  Castmagic  Castmagic is the ultimate tool for podcasters ...\n",
            "2   Z-Stream  Z-Stream is a full-service workflow tool, whic...\n",
            "3      Acute  Acute makes it easy for you to collect feedbac...\n",
            "The list of common products has been saved to 'common_productstry.csv'.\n"
          ]
        }
      ]
    }
  ]
}